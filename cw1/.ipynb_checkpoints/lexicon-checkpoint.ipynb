{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Lexicon:\n",
    "    \"\"\" A class that stores a collection of processed documents\n",
    "        and provides the methods of processing\"\"\"\n",
    "    \n",
    "    def __init__(self, xml_path, stop_words_path, stemmer):\n",
    "        self.docs_df = pd.DataFrame(columns=['doc'])\n",
    "        self.stop_words = []  \n",
    "        self.stemmer = stemmer\n",
    "        self.index = defaultdict(lambda: defaultdict(list))  # positional inverted index\n",
    "        self.load_xml(xml_path)\n",
    "        self.load_stop_words(stop_words_path)\n",
    "        \n",
    "        \n",
    "    def load_xml(self, file):\n",
    "        \"\"\"parse the XML file in standard TREC\n",
    "\n",
    "        Args:\n",
    "            file (str): path to the xml file\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Reading file\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            xml = f.read()\n",
    "\n",
    "        # Add a root tag\n",
    "        xml = '<ROOT>' + xml + '</ROOT>'\n",
    "\n",
    "        # build a dataframe to store each doc\n",
    "        print('Loading the XML file...')\n",
    "        for doc in ET.fromstring(xml):\n",
    "            docID = doc.find('DOCNO').text\n",
    "            # doc contains both HEADLINE and TEXT\n",
    "            content = doc.find('HEADLINE').text + doc.find('TEXT').text\n",
    "            self.docs_df.loc[docID] = content\n",
    "        print('Finished. ')\n",
    "            \n",
    "            \n",
    "    def load_stop_words(self, file):\n",
    "        with open(file, 'r', encoding='utf-8-sig') as f:\n",
    "            # remove \\n for each stop word\n",
    "            self.stop_words = [w.replace('\\n', '') for w in f.readlines()]\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        \"\"\"tokenize the input text (split at non word character)\n",
    "\n",
    "        Args:\n",
    "            text (str): the input text\n",
    "\n",
    "        Returns:\n",
    "            tokens\n",
    "\n",
    "        \"\"\"\n",
    "        pa = r'\\w+'\n",
    "        return re.findall(pa, text)\n",
    "            \n",
    "\n",
    "    def preprocess(self, text):\n",
    "        \"\"\"preprocess the text:\n",
    "           1. tokenization; \n",
    "           2. case folding;\n",
    "           3. stopwords cleaning\n",
    "           3. normalisation (stemming).\n",
    "\n",
    "        Args:\n",
    "            text (str): the input text \n",
    "\n",
    "        Returns:\n",
    "            [preprocessed tokens]\n",
    "\n",
    "        \"\"\"\n",
    "        # load the Porter Stemmer from nltk\n",
    "        tokens = [self.stemmer.stem(t.lower()) for t in self.tokenize(text) \\\n",
    "                        if not t.lower() in self.stop_words]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def pos_inv_ind(self):\n",
    "        \"\"\"build the positional inverted index\"\"\"\n",
    "        \n",
    "        print('Building the Positional Inverted Index...')\n",
    "        for i, dp in self.docs_df.iterrows():\n",
    "            doc = dp['doc']\n",
    "            tokens = self.preprocess(doc)\n",
    "            for pos in range(len(tokens)):\n",
    "                \"\"\"term:\n",
    "                       docID: pos1, pos2, ...\n",
    "                       docID: pos1, pos2, ...\n",
    "                \"\"\" \n",
    "                self.index[tokens[pos]][i] += [pos + 1]  # position starts from 1\n",
    "        # sort the index by keys\n",
    "        self.index = OrderedDict(sorted(self.index.items()))\n",
    "        print('Finished.') \n",
    "        \n",
    "        \n",
    "    def export_index(self):\n",
    "        \"\"\"export positional inverted index into readable format\"\"\"\n",
    "        \n",
    "        with open('index.txt', 'w+', encoding='utf-8') as f:\n",
    "            for term in self.index.keys():\n",
    "                    f.write(term + ':\\n')\n",
    "                    _dict_ = self.index[term]\n",
    "                    for docID in _dict_.keys():\n",
    "                        pos_list = _dict_[docID]\n",
    "                        f.write('\\t' + str(docID) + ': '\\\n",
    "                                + ','.join(str(pos) for pos in pos_list)\\\n",
    "                                + '\\n')\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] xml st\n",
      "ipykernel_launcher.py: error: the following arguments are required: st\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawhy\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--xml\", type=str, help=\"the path to the xml file\")\n",
    "    parser.add_argument(\"--st\", type=str, help=\"the path to the stop words list\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    lexicon = Lexicon(args.xml, args.st, PorterStemmer())\n",
    "    lexicon.pos_inv_ind()\n",
    "    with open('index.json', 'w+') as f:\n",
    "        json.dump(lexicon.index, f)\n",
    "    lexicon.export_index()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
